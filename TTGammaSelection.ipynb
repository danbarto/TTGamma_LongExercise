{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import coffea.processor as processor\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea import hist\n",
    "\n",
    "# this avoids some warnings due to using older NanoAOD ntuples\n",
    "NanoAODSchema.warn_missing_crossrefs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below introduces some basic concepts for writing code using Coffea.  \n",
    "\n",
    "There are three primary pieces to the Coffea code:  \n",
    "1. The *processor*, which contains all of the analysis cuts and fills the histogram in the _process_ function. \n",
    "2. The second cell defines the files we want to run over and then runs the code using run_uproot_job. \n",
    "3. After we run the processor, we can then plot any of the histograms we have generated.\n",
    "\n",
    "To test any changes you make to the histograms, you will have to rerun each of the three cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTGammaCutflow(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        ### This function is where the histograms are defined and any other initialization happens\n",
    "        \n",
    "        ### Muon pt\n",
    "        #Declare an axis for the dataset\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Dataset\")\n",
    "        \n",
    "        #Declare an axis for the muon pt\n",
    "        muon_pt_axis = hist.Bin(\"pt\", \"$p_{T}$ [GeV]\", 40, 0, 200)\n",
    "        \n",
    "\n",
    "        #Define the accumulator object, a dictionary storing all of the histograms and counters \n",
    "        #that we will fill later in the process function\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'muon_pt': hist.Hist(\"Counts\", dataset_axis, muon_pt_axis),\n",
    "            ######\n",
    "            ### Step 1. Add a histogram for the electron pt\n",
    "            ######\n",
    "        })\n",
    "        \n",
    "        ######\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        ### The process function is where most of the work happens. As we'll see below, this is\n",
    "        ### where the main analysis work happens (object cuts, event selections, filling histograms). \n",
    "        \n",
    "        ## This gets us the accumulator dictionary we defined in init\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        ## To access variables from the ntuples, use the \"events\" object\n",
    "        ## The dataset name is part of events.metadata\n",
    "        dataset = events.metadata['dataset']\n",
    "        \n",
    "        ## The coffea NanoEventSchema packages all muon variables (columns) into the events.Muon object\n",
    "        ## Each variable can be accessed using muons.key_name\n",
    "        muons = events.Muon        \n",
    "        \n",
    "        ######\n",
    "        \n",
    "        # Select muons with pt >30, eta < 2.4, tight ID, and relIso < 0.15\n",
    "        muonSelectTight = ((muons.pt>30) &\n",
    "                           (abs(muons.eta)<2.4) &\n",
    "                           (muons.tightId) &\n",
    "                           (muons.pfRelIso04_all < 0.15)\n",
    "                          )\n",
    "\n",
    "        # Apply the selection to muons using the array[mask] syntax. \n",
    "        # tightMuons only includes the muons that pass the tight selection we defined\n",
    "        tightMuons = muons[muonSelectTight]\n",
    "        \n",
    "        ######\n",
    "        ### Step 2. Define your own selection for electrons. \n",
    "        ### Note that the ID variable names will be different. \n",
    "        ### Either remove the ID and iso variables or replace them with the correct electron values.\n",
    "        ###### \n",
    "\n",
    "        ######\n",
    "        \n",
    "        # Select events with exactly one tight muon. \n",
    "        eventSelection = (ak.num(tightMuons)==1)\n",
    "        \n",
    "        ######\n",
    "        ### Step 3. Define a second event selection requiring events with no muons and exactly one electron\n",
    "        ###### \n",
    "\n",
    "        ######\n",
    "\n",
    "        # Fill the muon_pt histogram using the tightMuons in events that pass our selection \n",
    "        # Note that ak.flatten() is required when filling a histogram to remove the jaggedness\n",
    "        output['muon_pt'].fill(dataset=dataset,\n",
    "                              pt=ak.flatten(tightMuons[eventSelection].pt))\n",
    "        \n",
    "        ######\n",
    "        ### Step 4. Fill the ele_pt histogram you defined earlier\n",
    "        ###### \n",
    "\n",
    "        \n",
    "        ######\n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files to run over\n",
    "fileset = {\n",
    "    \"TTGamma\": [\n",
    "        \"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTGamma_1l.root\"\n",
    "    ],\n",
    "    \"TTbar\": [\n",
    "        \"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTbar_1l.root\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Run Coffea code using uproot\n",
    "output = processor.run_uproot_job(\n",
    "    fileset,\n",
    "    \"Events\",\n",
    "    TTGammaCutflow(),\n",
    "    processor.iterative_executor,\n",
    "    {\"schema\": NanoAODSchema},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the muon_pt histogram, overlaying the distribution for each dataset\n",
    "# Sometimes have to run this cell twice for the plot to appear\n",
    "hist.plot1d(output['muon_pt'], overlay='dataset')\n",
    "\n",
    "\n",
    "######\n",
    "### Step 5. Plot the ele_pt histogram\n",
    "###### \n",
    "        \n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to look at other files, feel free to edit _fileset_ above using the skimmed datasets here: /store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen some of the basics of histogramming, object cuts, and event selection, let's go through how to apply weights to the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from coffea.lookup_tools import dense_lookup\n",
    "from coffea.analysis_tools import Weights\n",
    "\n",
    "class TTGammaWeights(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # There are several utilities within coffea that can help us apply weights and systematics to the code\n",
    "        # We use uproot to open the root file containing the electron ID scale factors:\n",
    "        ele_id_file = uproot.open('ttgamma/ScaleFactors/MuEGammaScaleFactors/ele2016/2016LegacyReReco_ElectronTight_Fall17V2.root')\n",
    "        # The dense_lookup tools in Coffea make it easy to extract the histogram (named EGamma_SF2D in this case)\n",
    "        # with the weights and their errors:\n",
    "        self.ele_id_sf = dense_lookup.dense_lookup(ele_id_file[\"EGamma_SF2D\"].values(), \n",
    "                                                   (ele_id_file[\"EGamma_SF2D\"].axis(0).edges(), ele_id_file[\"EGamma_SF2D\"].axis(1).edges()))\n",
    "        self.ele_id_err = dense_lookup.dense_lookup(ele_id_file[\"EGamma_SF2D\"].variances()**0.5, \n",
    "                                                    (ele_id_file[\"EGamma_SF2D\"].axis(0).edges(), ele_id_file[\"EGamma_SF2D\"].axis(1).edges()))\n",
    "\n",
    "\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Dataset\")\n",
    "\n",
    "        # The systematic axis will be used to keep track of whether we are using the nominal weights\n",
    "        # or the weights shifted up/down by their errors\n",
    "        systematic_axis = hist.Cat(\"systematic\", \"Systematic Uncertainty\")\n",
    "        ele_pt_axis = hist.Bin(\"pt\",\"$p_{T}$ [GeV]\", 40, 0, 200)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'ele_pt': hist.Hist(\"Counts\", dataset_axis, systematic_axis, ele_pt_axis)\n",
    "        }\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "\n",
    "        electrons = events.Electron\n",
    "        \n",
    "        #Define tight electron selection\n",
    "        electronSelectTight = ((electrons.pt>35) & \n",
    "                               (abs(electrons.eta)<2.1) & \n",
    "                               ((abs(electrons.eta) < 1.4442) | (abs(electrons.eta) > 1.566)) &      \n",
    "                               (electrons.cutBased>=4)\n",
    "                              )\n",
    "        # Apply selection\n",
    "        tightElectron = electrons[electronSelectTight]\n",
    "        #Define event selection\n",
    "        eventSelection = (ak.num(tightElectron) ==1)\n",
    "\n",
    "\n",
    "        # Here, we look up the scale factors and errors for each electron using the electron eta and pt\n",
    "        eleID = self.ele_id_sf(tightElectron.eta, tightElectron.pt)\n",
    "        eleIDerr = self.ele_id_err(tightElectron.eta, tightElectron.pt)\n",
    "        \n",
    "        #To get an event-level weight, multiply the SF for each electron in the event\n",
    "        #The axis=-1 option means we are multiplying the innermost values (electrons per event in this case)\n",
    "        eleSF = ak.prod(eleID, axis=-1)\n",
    "        eleSFUp = ak.prod(eleID+eleIDerr, axis=-1)\n",
    "        eleSFDown = ak.prod(eleID-eleIDerr, axis=-1)\n",
    "    \n",
    "        # The Weights object is a container that handles the bookkeeping \n",
    "        # for event weights and associated systematic shifts.\n",
    "        # The argument for the Weights object is the number of events we are processing\n",
    "        weights = Weights(len(events))\n",
    "\n",
    "        # Add the ele ID SF to the weights object\n",
    "        weights.add('eleEffWeight',weight=eleSF,weightUp=eleSFUp,weightDown=eleSFDown)\n",
    "        \n",
    "        systList = ['noweight','nominal','eleEffWeightUp','eleEffWeightDown']\n",
    "\n",
    "        for syst in systList:\n",
    "           \n",
    "            weightSyst = syst\n",
    "            if syst=='nominal':\n",
    "                weightSyst=None\n",
    "                \n",
    "            if syst=='noweight':\n",
    "                evtWeight = np.ones(len(events))\n",
    "            else:\n",
    "                evtWeight = weights.weight(weightSyst)\n",
    "                \n",
    "            output['ele_pt'].fill(dataset=dataset,\n",
    "                                  systematic=syst,\n",
    "                                  pt=ak.flatten(tightElectron[eventSelection].pt),\n",
    "                                  weight=evtWeight[eventSelection])\n",
    "           \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files to run over\n",
    "fileset = {\n",
    "    \"TTGamma\": [\n",
    "        \"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTGamma_1l.root\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# Run Coffea code using uproot\n",
    "output = processor.run_uproot_job(\n",
    "    fileset,\n",
    "    \"Events\",\n",
    "    TTGammaWeights(),\n",
    "    processor.iterative_executor,\n",
    "    {\"schema\": NanoAODSchema},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pt distributions for each of the different systematics\n",
    "hist.plot1d(output['ele_pt'].sum('dataset'), overlay='systematic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
